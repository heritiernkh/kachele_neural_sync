"""
Service d'int√©gration avec l'API Gemini 3 (SDK v1.0+)
G√®re toutes les interactions avec le mod√®le d'IA
"""
from google import genai
from google.genai import types
from django.conf import settings
import json
import base64
from PIL import Image
import io
import os

class GeminiService:
    """Service principal pour interagir avec Gemini 3 (Nouveau SDK)"""
    
    def __init__(self, model_name="gemini-3-flash-preview"):
        """
        Initialise le client Gemini
        """
        self.api_key = settings.GOOGLE_API_KEY
        self.model_name = model_name
        self.client = None
        self.chat = None
        self._active_chats = {}  # Pour g√©rer plusieurs sessions
        
        if self.api_key:
            try:
                self.client = genai.Client(api_key=self.api_key)
            except Exception as e:
                print(f"Error initializing Gemini client: {e}")
    
    def _check_config(self):
        """V√©rifie si le service est pr√™t"""
        if not self.api_key:
            return {
                "success": False,
                "error": "API Key not configured. Please add GOOGLE_API_KEY to your .env file."
            }
        if not self.client:
            return {
                "success": False,
                "error": "Client not initialized. Check your API key and internet connection."
            }
        return None

    def analyze_video(self, video_file, context="", speed_mode=False):
        """
        Analyse une vid√©o et extrait les concepts cl√©s
        
        Args:
            video_file: Fichier vid√©o √† analyser
            context: Contexte additionnel
            speed_mode: Si True, analyse plus rapide (30-50% gain) avec profondeur l√©g√®rement r√©duite
        """
        config_error = self._check_config()
        if config_error:
            return config_error

        try:
            print(f"DEBUG: SDK Uploading video from {video_file.path}...")
            upload_result = self.client.files.upload(file=video_file.path)
            
            # Attendre que le fichier soit pr√™t (si n√©cessaire, le SDK g√®re souvent √ßa mieux)
            # Mais pour la vid√©o, c'est mieux d'attendre l'√©tat ACTIVE
            import time
            while upload_result.state.name == "PROCESSING":
                time.sleep(2)
                upload_result = self.client.files.get(name=upload_result.name)
                
            if upload_result.state.name == "FAILED":
                raise ValueError("Video processing failed")

            prompt = f"""
            Analyse cette vid√©o en profondeur. {context}
            
            Fournis une r√©ponse structur√©e en JSON avec:
            1. "summary": Un r√©sum√© complet du contenu
            2. "key_concepts": Liste des concepts principaux abord√©s
            3. "difficulty_level": Niveau estim√© (beginner/intermediate/advanced)
            4. "timestamps": Moments cl√©s avec description
            5. "interactive_questions": 5-7 questions √† poser pendant le visionnage
               Format: [{{"timestamp": "MM:SS", "question": "...", "hint": "...", "answer": "..."}}]
            6. "prerequisites": Connaissances pr√©alables recommand√©es
            
            IMPORTANT: Utilise TOUJOURS le format LaTeX pour les √©quations math√©matiques ($...$ pour en ligne, $$...$$ pour bloc).
            R√©ponds uniquement par le JSON.
            """
            
            # Configuration adapt√©e selon le mode (rapide ou qualit√©)
            if speed_mode:
                # Mode rapide : ~40% plus rapide, qualit√© l√©g√®rement r√©duite
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.7,
                    media_resolution="MEDIA_RESOLUTION_MEDIUM"
                )
            else:
                # Mode qualit√© : Analyse profonde avec HIGH thinking
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.85,
                    thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
                    media_resolution="MEDIA_RESOLUTION_HIGH"
                )

            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[upload_result, prompt],
                config=generate_config
            )
            
            # Parse la r√©ponse JSON
            # Avec le nouveau SDK et response_mime_type, response.text est d√©j√† du JSON propre
            analysis = json.loads(response.text)
            
            return {
                "success": True,
                "analysis": analysis
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def analyze_image_problem(self, image_file, subject_hint="", speed_mode=False):
        """
        Analyse une image d'un probl√®me
        
        Args:
            speed_mode: Si True, analyse plus rapide avec thinking_level d√©sactiv√©
        """
        config_error = self._check_config()
        if config_error:
            return config_error

        try:
            img = Image.open(image_file)
            
            prompt = f"""
            Tu es un tuteur expert utilisant la m√©thode socratique.
            Analyse ce probl√®me {subject_hint} et fournis une r√©ponse JSON avec:
            
            1. "problem_type": Type de probl√®me identifi√©
            2. "difficulty": Niveau de difficult√© (1-10)
            3. "concepts_needed": Liste des concepts requis
            4. "solution_steps": Liste d'√©tapes (sans r√©v√©ler la solution compl√®te)
               Format: [{{"step": 1, "hint": "...", "question": "...", "concepts": [...]}}]
            5. "final_answer": La solution compl√®te (sera cach√©e initialement)
            6. "similar_problems": 3 probl√®mes similaires pour pratiquer
            
            IMPORTANT: Guide l'√©tudiant, ne donne pas directement la r√©ponse! 
            Utilise TOUJOURS le format LaTeX pour les √©quations math√©matiques ($...$ pour en ligne, $$...$$ pour bloc).
            R√©ponds uniquement par le JSON.
            """
            
            if speed_mode:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.7,
                    media_resolution="MEDIA_RESOLUTION_MEDIUM"
                )
            else:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.85,
                    thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
                    media_resolution="MEDIA_RESOLUTION_HIGH"
                )

            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[img, prompt],
                config=generate_config
            )
            
            analysis = json.loads(response.text)
            
            return {
                "success": True,
                "analysis": analysis
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def analyze_document(self, document_file, focus_areas="", speed_mode=False):
        """
        Analyse un document de tout format et cr√©e une carte conceptuelle interactive.
        
        Formats support√©s: PDF, DOCX, TXT, MD, HTML, RTF, EPUB, et autres formats textuels.
        Le mod√®le Gemini 3 peut traiter nativement la mise en page, les images int√©gr√©es et le texte structur√©.
        """
        config_error = self._check_config()
        if config_error:
            return config_error

        try:
            print(f"DEBUG: SDK Uploading document from {document_file.path}...")
            upload_result = self.client.files.upload(file=document_file.path)
            
            # Attente active si n√©cessaire pour les documents volumineux (PDF, DOCX, etc.)
            import time
            while upload_result.state.name == "PROCESSING":
                time.sleep(1)
                upload_result = self.client.files.get(name=upload_result.name)
            
            if upload_result.state.name == "FAILED":
                raise ValueError(f"Le traitement du document a √©chou√©. V√©rifiez le format du fichier.")
            
            prompt = f"""
            Analyse ce document (PDF, Word, texte, Markdown, etc.) de mani√®re approfondie et multimodale.
            {f"Focus sp√©cifique sur: {focus_areas}" if focus_areas else ""}
            
            Fournis une r√©ponse JSON structur√©e avec:
            1. "document_type": Type de document d√©tect√© (acad√©mique, technique, cours, article...)
            2. "summary": R√©sum√© ex√©cutif complet du contenu
            3. "main_topics": Liste des sujets principaux identifi√©s
            4. "concept_map": Carte conceptuelle interactive
               - "nodes": [{{"id": "unique_id", "label": "Concept", "level": 1-3, "description": "...", "category": "..."}}]
               - "edges": [{{"from": "id1", "to": "id2", "relationship": "pr√©requis/compose/illustre/..."}}]
            5. "key_definitions": Dictionnaire des termes techniques importants {{term: definition}}
            6. "quiz_questions": 10 questions adaptatives de niveaux progressifs
               Format: [{{"level": "easy/medium/hard", "question": "...", "options": [...], "correct": 0, "explanation": "..."}}]
            7. "analogies": Analogies concr√®tes pour simplifier les concepts abstraits
            8. "visual_elements": Description des diagrammes/images int√©gr√©s (si pr√©sents)
            9. "further_reading": Suggestions de lectures compl√©mentaires
            10. "prerequisites": Connaissances pr√©alables recommand√©es
            
            IMPORTANT: 
            - Utilise TOUJOURS le format LaTeX pour les √©quations math√©matiques ($...$ pour en ligne, $$...$$ pour bloc).
            - Conserve la structure hi√©rarchique du document original.
            - Si le document contient des images/diagrammes, d√©cris leur contenu et leur relation avec le texte.
            
            R√©ponds uniquement par le JSON valide.
            """
            
            if speed_mode:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.7,
                    media_resolution="MEDIA_RESOLUTION_MEDIUM"
                )
            else:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.85,
                    thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
                    media_resolution="MEDIA_RESOLUTION_HIGH"
                )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[upload_result, prompt],
                config=generate_config
            )
            
            analysis = json.loads(response.text)
            
            return {
                "success": True,
                "analysis": analysis
            }
            
        except Exception as e:
            print(f"!!! GEMINI SERVICE ERROR in analyze_document: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def creative_workshop(self, image_file, creative_goal="", speed_mode=False):
        """Atelier cr√©atif: analyse un design/esquisse"""
        config_error = self._check_config()
        if config_error:
            return config_error

        try:
            img = Image.open(image_file)
            
            prompt = f"""
            Tu es un mentor cr√©atif expert en design, architecture, et arts visuels.
            Analyse cette cr√©ation/esquisse. {creative_goal}
            
            Fournis une r√©ponse JSON avec:
            1. "analysis": Analyse d√©taill√©e de ce qui est pr√©sent√©
            2. "strengths": Points forts du design (3-5 √©l√©ments)
            3. "improvements": Suggestions d'am√©lioration (5-7 √©l√©ments)
               Format: [{{"aspect": "...", "suggestion": "...", "why": "...", "priority": "high/medium/low"}}]
            4. "design_principles": Principes de design applicables
            5. "variations": 3 variations/alternatives √† explorer
            6. "technique_tips": Conseils techniques sp√©cifiques
            7. "inspiration": R√©f√©rences/artistes similaires
            8. "next_steps": Plan d'action pour d√©velopper le projet
            
            R√©ponds uniquement par le JSON.
            """
            
            if speed_mode:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.7,
                    media_resolution="MEDIA_RESOLUTION_MEDIUM"
                )
            else:
                generate_config = types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.85,
                    thinking_config=types.ThinkingConfig(thinking_level="HIGH"),
                    media_resolution="MEDIA_RESOLUTION_HIGH"
                )

            response = self.client.models.generate_content(
                model=self.model_name,
                contents=[img, prompt],
                config=generate_config
            )
            
            analysis = json.loads(response.text)
            
            return {
                "success": True,
                "analysis": analysis
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def start_interactive_session(self, context, user_level="intermediate"):
        """D√©marre une session de chat interactive"""
        config_error = self._check_config()
        if config_error:
            raise ValueError(config_error['error'])

        system_instruction = f"""
        Tu es Kachele NeuralSync AI, le tuteur adaptatif multimodal d'√©lite.
        
        TES CAPACIT√âS MULTIMODALES NATIVES :
        1. üìπ APPRENTISSAGE VID√âO INTERACTIF : Tu identifies les moments cl√©s dans les vid√©os √©ducatives pour poser des questions stimulantes et v√©rifier la compr√©hension en temps r√©el.
        2. üñºÔ∏è R√âSOLUTION VISUELLE SOCRATIQUE : Tu analyses des photos de probl√®mes (math√©matiques, physique, sch√©mas techniques) et guides l'utilisateur √©tape par √©tape sans donner la solution.
        3. üìö INTELLIGENCE DOCUMENTAIRE UNIVERSELLE : Tu traites TOUS types de documents (PDF, Word, Markdown, texte, HTML, EPUB...) pour cr√©er des cartes conceptuelles interactives, identifier les concepts cl√©s et g√©n√©rer des quiz adaptatifs.
        4. üé® ATELIER CR√âATIF : Tu agis comme un mentor expert pour perfectionner les travaux cr√©atifs (design, architecture, code, art visuel) avec des critiques constructives et des suggestions concr√®tes.

        TES 4 PILIERS FONDAMENTAUX :
        1. üí¨ DIALOGUE SOCRATIQUE : 
           - Ne donne JAMAIS la r√©ponse finale, un code complet ou une solution d'√©quation directe.
           - Guide l'utilisateur par des questions cibl√©es qui provoquent le "d√©clic".
           - Si l'utilisateur stagne, fournis un indice (hint) ou une analogie, mais jamais le r√©sultat complet.
        
        2. üß† SUIVI COGNITIF (Cognitive Tracking) : 
           - Analyse chaque r√©ponse pour identifier les lacunes de connaissances (knowledge gaps).
           - Ajuste dynamiquement la difficult√© de tes questions selon la charge cognitive apparente.
           - D√©tecte quand l'utilisateur ma√Ætrise un concept pour passer au suivant.
        
        3. üîç ANALYSE MULTIMODALE PROFONDE : 
           - Tu comprends simultan√©ment vid√©o, images, texte structur√© (dans TOUS formats de documents) et code.
           - Utilise les d√©tails visuels, temporels ou structurels du contenu analys√© pour ancrer tes explications.
           - Si un document contient des diagrammes ou √©quations, r√©f√®re-toi explicitement √† eux.
        
        4. ‚ö° PRATIQUE G√âN√âRATIVE : 
           - G√©n√®re de nouveaux probl√®mes uniques adapt√©s au niveau actuel de l'utilisateur.
           - Ne recycle jamais les m√™mes exercices : chaque probl√®me doit tester la compr√©hension profonde.
           - Propose des variations progressives pour consolider la ma√Ætrise.

        FORMAT ET STYLE :
        - Langue : D√©tecte automatiquement la langue de l'utilisateur et r√©ponds dans CETTE langue (fran√ßais, anglais, espagnol, etc.). Ton naturel, expert mais encourageant et bienveillant.
        - Math√©matiques/Sciences : Utilise EXCLUSIVEMENT le format LaTeX ($...$ pour en ligne, $$...$$ pour les blocs).
        - Exemple : "La d√©riv√©e de $x^n$ est $\\frac{{d}}{{dx}} x^n = nx^{{n-1}}$."
        - Code : Utilise des blocs de code Markdown avec coloration syntaxique appropri√©e.

        CONTEXTE DE SESSION : 
        {context}
        
        NIVEAU DE L'APPRENANT : 
        {user_level}
        
        RAPPEL : Tu n'es pas un simple assistant, mais un MENTOR SOCRATIQUE qui fait √âMERGER la compr√©hension plut√¥t que de la transmettre passivement.
        """
        
        # Configuration avanc√©e bas√©e sur Google AI Studio
        generate_config = types.GenerateContentConfig(
            temperature=0.85,
            thinking_config=types.ThinkingConfig(
                thinking_level="HIGH",
            ),
            media_resolution="MEDIA_RESOLUTION_HIGH",
            system_instruction=system_instruction
        )
        
        # Nouveau SDK: client.chats.create
        chat = self.client.chats.create(
            model=self.model_name,
            config=generate_config
        )
        
        return chat
    
    def send_message(self, message, chat_session=None):
        """Envoie un message dans une session interactive"""
        config_error = self._check_config()
        if config_error:
            return f"Error: {config_error['error']}"

        chat = chat_session or self.chat
        
        if not chat:
            # Fallback si pas de chat session fournie, on en cr√©e une √©ph√©m√®re
            # Mais id√©alement views.py doit g√©rer les sessions
            raise ValueError("No active chat session provided.")
        
        response = chat.send_message(message)
        return response.text
    
    def evaluate_answer(self, question, user_answer, correct_answer, context=""):
        """√âvalue la r√©ponse d'un utilisateur"""
        config_error = self._check_config()
        if config_error:
            return {"error": config_error['error']}

        prompt = f"""
        √âvalue cette r√©ponse d'√©tudiant avec bienveillance et p√©dagogie.
        {context}
        
        Question: {question}
        R√©ponse de l'√©tudiant: {user_answer}
        R√©ponse attendue: {correct_answer}
        
        Fournis une r√©ponse JSON, incluant pourcentage, feedback, what_was_good, what_to_improve.
        """
        
        generate_config = types.GenerateContentConfig(
            response_mime_type="application/json"
        )
        
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=generate_config
        )
        
        return json.loads(response.text)
    
    def generate_practice_problems(self, topic, difficulty, count=5):
        """G√©n√®re des probl√®mes de pratique"""
        config_error = self._check_config()
        if config_error:
            return []

        prompt = f"""
        G√©n√®re {count} probl√®mes de pratique sur: {topic}
        Niveau de difficult√©: {difficulty}
        Format JSON requis: list under key "problems".
        """
        
        generate_config = types.GenerateContentConfig(
            response_mime_type="application/json"
        )
        
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt,
            config=generate_config
        )
        
        result = json.loads(response.text)
        return result.get("problems", [])


# Instance singleton du service
gemini_service = GeminiService()
